{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import make_grid, save_image\n",
        "import torch.distributions as dist\n",
        "import os\n",
        "import numpy as np\n",
        "import errno\n",
        "import os\n",
        "import PIL\n",
        "from functools import reduce\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import FashionMNIST\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "p2s_d6XbzsMY"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class View(nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super(View, self).__init__()\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, tensor):\n",
        "        return tensor.view(self.size)\n",
        "\n",
        "class FashionMNISTEncoder(nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super().__init__()\n",
        "        hidden_dim=256\n",
        "        self.z_dim = z_dim\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 4, 2, 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 32, 4, 2, 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(128, hidden_dim, 4, 1),\n",
        "            nn.ReLU(True),\n",
        "            View((-1, hidden_dim*1*1)),\n",
        "            nn.Linear(hidden_dim, z_dim*2),\n",
        "        )\n",
        "\n",
        "        self.locs = nn.Linear(hidden_dim, z_dim)\n",
        "        self.scales = nn.Linear(hidden_dim, z_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.encoder(x)\n",
        "        return hidden[:, :self.z_dim], torch.clamp(F.softplus(hidden[:, self.z_dim:]), min=1e-3)\n",
        "\n",
        "\n",
        "class FashionMNISTDecoder(nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super().__init__()\n",
        "        hidden_dim=256\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, hidden_dim),\n",
        "            View((-1, hidden_dim, 1, 1)),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(hidden_dim, 128, 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 32, 4, 2, 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 1, 4, 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, z_dim, classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        h_dim = 500\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(z_dim, classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "class ConditionalPrior(nn.Module):\n",
        "    def __init__(self, z_dim, classes):\n",
        "        super(ConditionalPrior, self).__init__()\n",
        "        h_dim = 500\n",
        "        self.locs = nn.Linear(classes, z_dim)\n",
        "        self.scales = nn.Linear(classes, z_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.locs(x), torch.clamp(F.softplus(self.scales(x)), min=1e-3)"
      ],
      "metadata": {
        "id": "dCrAGwbP7rIV"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utility Functions"
      ],
      "metadata": {
        "id": "4JH0vmQiI-KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_kl(locs_q, scale_q, locs_p=None, scale_p=None):\n",
        "    \"\"\"\n",
        "    Computes the KL(q||p)\n",
        "    \"\"\"\n",
        "    if locs_p is None:\n",
        "        locs_p = torch.zeros_like(locs_q)\n",
        "    if scale_p is None:\n",
        "        scale_p = torch.ones_like(scale_q)\n",
        "\n",
        "    dist_q = dist.Normal(locs_q, scale_q)\n",
        "    dist_p = dist.Normal(locs_p, scale_p)\n",
        "    return dist.kl.kl_divergence(dist_q, dist_p).sum(dim=-1)\n",
        "\n",
        "# Computes the log-likelihood of reconstructed images given the original images\n",
        "def img_log_likelihood(recon, xs):\n",
        "        return dist.Laplace(recon, torch.ones_like(recon)).log_prob(xs).sum(dim=(1,2,3))"
      ],
      "metadata": {
        "id": "6nbOiG3GJV79"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CCVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    CCVAE\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim, num_classes,\n",
        "                 im_shape, use_cuda, prior_fn):\n",
        "        super(CCVAE, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        #self.z_classify = num_classes\n",
        "        #self.z_style = z_dim - num_classes\n",
        "        self.z_classify = z_dim\n",
        "        self.z_style = 0\n",
        "        self.im_shape = im_shape\n",
        "        self.use_cuda = use_cuda\n",
        "        self.num_classes = num_classes\n",
        "        self.ones = torch.ones(1, self.z_style)\n",
        "        self.zeros = torch.zeros(1, self.z_style)\n",
        "        self.y_prior_params = FashionMNISTCached.prior_fn()\n",
        "\n",
        "        self.encoder = FashionMNISTEncoder(self.z_dim)\n",
        "        self.decoder = FashionMNISTDecoder(self.z_dim)\n",
        "        self.classifier = Classifier(z_dim=self.z_dim, classes=self.num_classes)\n",
        "        self.cond_prior = ConditionalPrior(z_dim=self.z_dim, classes=self.num_classes)\n",
        "\n",
        "        if self.use_cuda:\n",
        "            self.ones = self.ones.cuda()\n",
        "            self.zeros = self.zeros.cuda()\n",
        "            self.y_prior_params = self.y_prior_params.cuda()\n",
        "            self.cuda()\n",
        "\n",
        "    def unsup(self, x):\n",
        "        bs = x.shape[0]\n",
        "        #inference\n",
        "        post_params = self.encoder(x)\n",
        "        z = dist.Normal(*post_params).rsample()\n",
        "        zc, zs = z.split([self.z_classify, self.z_style], 1)\n",
        "        qyzc = dist.Categorical(logits=self.classifier(zc))\n",
        "        y = qyzc.sample()\n",
        "        log_qy = qyzc.log_prob(y).sum(dim=-1)\n",
        "\n",
        "        # compute kl\n",
        "        y = F.one_hot(y, num_classes=self.num_classes).float()\n",
        "        locs_p_zc, scales_p_zc = self.cond_prior(y)\n",
        "        prior_params = (torch.cat([locs_p_zc, self.zeros.expand(bs, -1)], dim=1),\n",
        "                        torch.cat([scales_p_zc, self.ones.expand(bs, -1)], dim=1))\n",
        "        kl = compute_kl(*post_params, *prior_params)\n",
        "\n",
        "        #compute log probs for x and y\n",
        "        recon = self.decoder(z)\n",
        "        log_py = dist.Bernoulli(self.y_prior_params.expand(bs, -1)).log_prob(y).sum(dim=-1)\n",
        "        elbo = (img_log_likelihood(recon, x) + log_py - kl - log_qy).mean()\n",
        "        return -elbo\n",
        "\n",
        "    def sup(self, x, y):\n",
        "        bs = x.shape[0]\n",
        "        #inference\n",
        "        post_params = self.encoder(x)\n",
        "        z = dist.Normal(*post_params).rsample()\n",
        "        zc, zs = z.split([self.z_classify, self.z_style], 1)\n",
        "        qyzc = dist.Categorical(logits=self.classifier(zc))\n",
        "        log_qyzc = qyzc.log_prob(y).sum(dim=-1)\n",
        "\n",
        "        # compute kl\n",
        "        y = F.one_hot(y, num_classes=self.num_classes).float()\n",
        "        locs_p_zc, scales_p_zc = self.cond_prior(y)\n",
        "        prior_params = (torch.cat([locs_p_zc, self.zeros.expand(bs, -1)], dim=1),\n",
        "                        torch.cat([scales_p_zc, self.ones.expand(bs, -1)], dim=1))\n",
        "        #prior_params = (self.zeros.expand(bs, -1), self.ones.expand(bs, -1))\n",
        "        kl = compute_kl(*post_params, *prior_params)\n",
        "\n",
        "        #compute log probs for x and y\n",
        "        recon = self.decoder(z)\n",
        "        log_py = dist.Bernoulli(self.y_prior_params.expand(bs, -1)).log_prob(y).sum(dim=-1)\n",
        "        log_qyx = self.classifier_loss(x, y)\n",
        "        log_pxz = img_log_likelihood(recon, x)\n",
        "\n",
        "        # we only want gradients wrt to params of qyz, so stop them propogating to qzx\n",
        "        log_qyzc_ = dist.Bernoulli(logits=self.classifier(zc.detach())).log_prob(y).sum(dim=-1)\n",
        "        w = torch.exp(log_qyzc_ - log_qyx)\n",
        "        elbo = (w * (log_pxz - kl - log_qyzc) + log_py + log_qyx).mean()\n",
        "        return -elbo\n",
        "\n",
        "    def classifier_loss(self, x, y, k=100):\n",
        "        \"\"\"\n",
        "        Computes the classifier loss.\n",
        "        \"\"\"\n",
        "        zc, _ = dist.Normal(*self.encoder(x)).rsample(torch.tensor([k])).split([self.z_classify, self.z_style], -1)\n",
        "        logits = self.classifier(zc.view(-1, self.z_classify))\n",
        "        d = dist.Bernoulli(logits=logits)\n",
        "        y = y.expand(k, -1, -1).contiguous().view(-1, self.num_classes)\n",
        "        lqy_z = d.log_prob(y).view(k, x.shape[0], self.num_classes).sum(dim=-1)\n",
        "        lqy_x = torch.logsumexp(lqy_z, dim=0) - np.log(k)\n",
        "        return lqy_x\n",
        "\n",
        "    def reconstruct_img(self, x):\n",
        "        return self.decoder(dist.Normal(*self.encoder(x)).rsample())\n",
        "\n",
        "    def classifier_acc(self, x, y=None, k=1):\n",
        "        zc, _ = dist.Normal(*self.encoder(x)).rsample(torch.tensor([k])).split([self.z_classify, self.z_style], -1)\n",
        "        logits = self.classifier(zc.view(-1, self.z_classify)).view(-1, self.num_classes)\n",
        "        #y = y.expand(k, -1,-1).contiguous().view(-1, self.num_classes)\n",
        "        #y = y.expand(k, -1).contiguous()\n",
        "        preds = torch.softmax(logits, dim=1)\n",
        "        #acc = (preds.eq(y)).float().mean()\n",
        "        acc = (torch.max(preds,dim=1).indices.eq(y)).float().mean()\n",
        "        return acc\n",
        "\n",
        "    def save_models(self, path='./data'):\n",
        "        torch.save(self.encoder, os.path.join(path,'encoder.pt'))\n",
        "        torch.save(self.decoder, os.path.join(path,'decoder.pt'))\n",
        "        torch.save(self.classifier, os.path.join(path,'classifier.pt'))\n",
        "        torch.save(self.cond_prior, os.path.join(path,'cond_prior.pt'))\n",
        "\n",
        "    def accuracy(self, data_loader, *args, **kwargs):\n",
        "        acc = 0.0\n",
        "        for (x, y) in data_loader:\n",
        "            if self.use_cuda:\n",
        "                x, y = x.cuda(), y.cuda()\n",
        "            batch_acc = self.classifier_acc(x, y)\n",
        "            acc += batch_acc\n",
        "        return acc / len(data_loader)\n",
        "\n",
        "    def latent_walk(self, images, save_dir):\n",
        "        #img_1=images[0]\n",
        "        img_2=images[3]\n",
        "        img_3=images[20]\n",
        "        #img_4=images[40]\n",
        "        latent_2=dist.Normal(*self.encoder(img_2)).sample()\n",
        "        latent_3=dist.Normal(*self.encoder(img_3)).sample()\n",
        "        num_points = 8\n",
        "        interpolated_points = [torch.lerp(latent_2, latent_3, i/(num_points+1)) for i in range(1, num_points+1)]\n",
        "        img_recon=[]\n",
        "        for point in interpolated_points:\n",
        "            img_recon.append(torch.squeeze(self.decoder(point).view(-1, *self.im_shape),dim=1))\n",
        "        grid_recon = make_grid(img_recon)\n",
        "        save_image(grid_recon, os.path.join(save_dir, \"test.png\"))\n",
        "\n",
        "    def latent_traversal(self, images, save_dir):\n",
        "        # Select four images for interpolation\n",
        "        img_1 = images[0]\n",
        "        img_2 = images[9]\n",
        "        img_3 = images[11]\n",
        "        img_4 = images[41]\n",
        "\n",
        "        # Encode the selected images to obtain latent vectors\n",
        "        latent_1 = dist.Normal(*self.encoder(img_1)).sample()\n",
        "        latent_2 = dist.Normal(*self.encoder(img_2)).sample()\n",
        "        latent_3 = dist.Normal(*self.encoder(img_3)).sample()\n",
        "        latent_4 = dist.Normal(*self.encoder(img_4)).sample()\n",
        "\n",
        "        # Perform interpolation between latent vectors for all four corners\n",
        "        num_points = 8\n",
        "        interpolated_latents = []\n",
        "        for i in range(num_points):\n",
        "            for j in range(num_points):\n",
        "                alpha = i / (num_points - 1)\n",
        "                beta = j / (num_points - 1)\n",
        "\n",
        "                interpolated_latent = (\n",
        "                    (1 - alpha) * (1 - beta) * latent_1 +\n",
        "                    alpha * (1 - beta) * latent_2 +\n",
        "                    (1 - alpha) * beta * latent_3 +\n",
        "                    alpha * beta * latent_4\n",
        "                )\n",
        "                interpolated_latents.append(interpolated_latent)\n",
        "\n",
        "        # Decode interpolated latent vectors and reconstruct images\n",
        "        img_recon=[]\n",
        "        for point in interpolated_latents:\n",
        "            img_recon.append(torch.squeeze(self.decoder(point).view(-1, *self.im_shape),dim=1))\n",
        "\n",
        "        # Create a grid of reconstructed images and save it\n",
        "        grid_recon = make_grid(img_recon, nrow=num_points)\n",
        "        save_image(grid_recon, os.path.join(save_dir, \"interpolation_result_bis.png\"))"
      ],
      "metadata": {
        "id": "H1VeqL78IIbk"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import errno\n",
        "import os\n",
        "import PIL\n",
        "from functools import reduce\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import FashionMNIST\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def split_fashionmnist(X, y, sup_frac, validation_num):\n",
        "    \"\"\"\n",
        "    splits FashionMNIST\n",
        "    \"\"\"\n",
        "\n",
        "    # validation set is the last 10,000 examples\n",
        "    X_valid = X[-validation_num:]\n",
        "    y_valid = y[-validation_num:]\n",
        "\n",
        "    X = X[0:-validation_num]\n",
        "    y = y[0:-validation_num]\n",
        "\n",
        "    if sup_frac == 0.0:\n",
        "        return None, None, X, y, X_valid, y_valid\n",
        "\n",
        "    if sup_frac == 1.0:\n",
        "        return X, y, None, None, X_valid, y_valid\n",
        "\n",
        "    split = int(sup_frac * len(X))\n",
        "    X_sup = X[0:split]\n",
        "    y_sup = y[0:split]\n",
        "    X_unsup = X[split:]\n",
        "    y_unsup = y[split:]\n",
        "\n",
        "    return X_sup, y_sup, X_unsup, y_unsup, X_valid, y_valid\n",
        "\n",
        "classes = 10\n",
        "\n",
        "FashionMNIST_classes = ('T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle_Boot')\n",
        "\n",
        "def custom_transform(x):\n",
        "    return x.float() / 255\n",
        "\n",
        "class FashionMNISTCached(FashionMNIST):\n",
        "    # static class variables for caching training data\n",
        "    classes = 10\n",
        "\n",
        "    train_data_sup, train_labels_sup = None, None\n",
        "    train_data_unsup, train_labels_unsup = None, None\n",
        "    train_data, test_labels = None, None\n",
        "    prior = torch.ones(1, classes) / classes\n",
        "    fixed_imgs = None\n",
        "    fixed_imgs_targets = None\n",
        "    validation_size = 20000\n",
        "    data_valid, labels_valid = None, None\n",
        "    shape = (1, 64, 64)\n",
        "\n",
        "    def prior_fn():\n",
        "        return FashionMNISTCached.prior\n",
        "\n",
        "    def clear_cache():\n",
        "        FashionMNISTCached.train_data, FashionMNISTCached.test_labels = None, None\n",
        "\n",
        "    def __init__(self, mode, sup_frac=None, *args, **kwargs):\n",
        "        super(FashionMNISTCached, self).__init__(train=True if mode in [\"sup\", \"unsup\", \"valid\"] else 'test', *args, **kwargs)\n",
        "        self.sub_label_inds = [i for i in range(classes)]\n",
        "        self.mode = mode\n",
        "\n",
        "        # self.transform = lambda x: (x/255).view(-1)\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.Resize(64),\n",
        "                transforms.Lambda(custom_transform)\n",
        "            ])\n",
        "\n",
        "        assert mode in [\"sup\", \"unsup\", \"test\", \"valid\"], \"invalid train/test option values\"\n",
        "\n",
        "        if mode in [\"sup\", \"unsup\", \"valid\"]:\n",
        "\n",
        "            if FashionMNISTCached.train_data is None:\n",
        "                print(\"Splitting Dataset\")\n",
        "\n",
        "                FashionMNISTCached.train_data = self.data\n",
        "                FashionMNISTCached.train_targets = self.targets\n",
        "\n",
        "                FashionMNISTCached.train_data_sup, FashionMNISTCached.train_labels_sup, \\\n",
        "                    FashionMNISTCached.train_data_unsup, FashionMNISTCached.train_labels_unsup, \\\n",
        "                    FashionMNISTCached.data_valid, FashionMNISTCached.labels_valid = \\\n",
        "                    split_fashionmnist(FashionMNISTCached.train_data, FashionMNISTCached.train_targets,\n",
        "                                 sup_frac, FashionMNISTCached.validation_size)\n",
        "\n",
        "            if mode == \"sup\":\n",
        "                self.data, self.targets = FashionMNISTCached.train_data_sup, FashionMNISTCached.train_labels_sup\n",
        "                FashionMNISTCached.prior = torch.mean(torch.nn.functional.one_hot(self.targets).float(), dim=0)\n",
        "            elif mode == \"unsup\":\n",
        "                self.data = FashionMNISTCached.train_data_unsup\n",
        "                # making sure that the unsupervised labels are not available to inference\n",
        "                self.targets = FashionMNISTCached.train_labels_unsup * np.nan\n",
        "            else:\n",
        "                self.data, self.targets = FashionMNISTCached.data_valid, FashionMNISTCached.labels_valid\n",
        "\n",
        "        else:\n",
        "            self.data = self.data\n",
        "            self.targets = self.targets\n",
        "\n",
        "        # create a batch of fixed images\n",
        "        if FashionMNISTCached.fixed_imgs is None:\n",
        "            temp = []\n",
        "            for i in range(64):\n",
        "                temp.append(self.transform(self.data[i, None, :, :]))\n",
        "            FashionMNISTCached.fixed_imgs = torch.stack(temp, dim=0)\n",
        "                #temp.append([self.transform(self.data[i, None,:,:]),self.targets[i]])\n",
        "            #FashionMNISTCached.fixed_imgs = temp\n",
        "\n",
        "        if FashionMNISTCached.fixed_imgs_targets is None:\n",
        "            temp = []\n",
        "            for i in range(64):\n",
        "                temp.append([self.transform(self.data[i, None,:,:]),self.targets[i]])\n",
        "            FashionMNISTCached.fixed_imgs_targets = temp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        X = self.transform(self.data[index, None,:,:])\n",
        "\n",
        "        target = self.targets[index]\n",
        "\n",
        "        return X, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def setup_data_loaders(use_cuda, batch_size, sup_frac=1.0, root=None, cache_data=False, **kwargs):\n",
        "    \"\"\"\n",
        "        helper function for setting up pytorch data loaders for a semi-supervised dataset\n",
        "    :param use_cuda: use GPU(s) for training\n",
        "    :param batch_size: size of a batch of data to output when iterating over the data loaders\n",
        "    :param sup_frac: fraction of supervised data examples\n",
        "    :param cache_data: saves dataset to memory, prevents reading from file every time\n",
        "    :param kwargs: other params for the pytorch data loader\n",
        "    :return: three data loaders: (supervised data for training, un-supervised data for training,\n",
        "                                  supervised data for testing)\n",
        "    \"\"\"\n",
        "\n",
        "    if root is None:\n",
        "        root = get_data_directory(__file__)\n",
        "    if 'num_workers' not in kwargs:\n",
        "        kwargs = {'num_workers': 2, 'pin_memory': True}\n",
        "    cached_data = {}\n",
        "    loaders = {}\n",
        "\n",
        "    #clear previous cache\n",
        "    FashionMNISTCached.clear_cache()\n",
        "\n",
        "    if sup_frac == 0.0:\n",
        "        modes = [\"unsup\", \"test\"]\n",
        "    elif sup_frac == 1.0:\n",
        "        modes = [\"sup\", \"test\", \"valid\"]\n",
        "    else:\n",
        "        modes = [\"unsup\", \"test\", \"sup\", \"valid\"]\n",
        "\n",
        "    for mode in modes:\n",
        "        cached_data[mode] = FashionMNISTCached(root=root, mode=mode, download=True, sup_frac=sup_frac)\n",
        "        loaders[mode] = DataLoader(cached_data[mode], batch_size=batch_size, shuffle=True, **kwargs)\n",
        "    return loaders"
      ],
      "metadata": {
        "id": "12VTy-1qjXe0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 200\n",
        "cuda = True\n",
        "sup_frac = 0.5\n",
        "z_dim = 45\n",
        "learning_rate = 2e-4\n",
        "data_dir = './data'\n",
        "num_epochs = 5"
      ],
      "metadata": {
        "id": "lXEOvM6ims0g"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    im_shape = FashionMNISTCached.shape\n",
        "\n",
        "    data_loaders = setup_data_loaders(cuda,\n",
        "                                      batch_size,\n",
        "                                      cache_data=True,\n",
        "                                      sup_frac=sup_frac,\n",
        "                                      root='')\n",
        "\n",
        "\n",
        "    cc_vae = CCVAE(z_dim=z_dim,\n",
        "                   num_classes=len(FashionMNIST_classes),\n",
        "                   im_shape=im_shape,\n",
        "                   use_cuda=cuda,\n",
        "                   prior_fn=data_loaders['test'].dataset.prior_fn)\n",
        "\n",
        "    optim = torch.optim.Adam(params=cc_vae.parameters(), lr=learning_rate)\n",
        "\n",
        "    # run inference for a certain number of epochs\n",
        "    for epoch in range(0, num_epochs):\n",
        "\n",
        "        # # # compute number of batches for an epoch\n",
        "        if sup_frac == 1.0: # fullt supervised\n",
        "            batches_per_epoch = len(data_loaders[\"sup\"])\n",
        "            period_sup_batches = 1\n",
        "            sup_batches = batches_per_epoch\n",
        "        elif sup_frac > 0.0: # semi-supervised\n",
        "            sup_batches = len(data_loaders[\"sup\"])\n",
        "            unsup_batches = len(data_loaders[\"unsup\"])\n",
        "            batches_per_epoch = sup_batches + unsup_batches\n",
        "            period_sup_batches = int(batches_per_epoch / sup_batches)\n",
        "        elif sup_frac == 0.0: # unsupervised\n",
        "            sup_batches = 0.0\n",
        "            batches_per_epoch = len(data_loaders[\"unsup\"])\n",
        "            period_sup_batches = np.Inf\n",
        "        else:\n",
        "            assert False, \"Data frac not correct\"\n",
        "\n",
        "        # initialize variables to store loss values\n",
        "        epoch_losses_sup = 0.0\n",
        "        epoch_losses_unsup = 0.0\n",
        "\n",
        "        # setup the iterators for training data loaders\n",
        "        if sup_frac != 0.0:\n",
        "            sup_iter = iter(data_loaders[\"sup\"])\n",
        "        if sup_frac != 1.0:\n",
        "            unsup_iter = iter(data_loaders[\"unsup\"])\n",
        "\n",
        "        # count the number of supervised batches seen in this epoch\n",
        "        ctr_sup = 0\n",
        "\n",
        "        for i in tqdm(range(batches_per_epoch)):\n",
        "            # whether this batch is supervised or not\n",
        "            is_supervised = (i % period_sup_batches == 0) and ctr_sup < sup_batches\n",
        "            # extract the corresponding batch\n",
        "            if is_supervised:\n",
        "                (xs, ys) = next(sup_iter)\n",
        "                ctr_sup += 1\n",
        "            else:\n",
        "                (xs, ys) = next(unsup_iter)\n",
        "\n",
        "            if cuda:\n",
        "                xs, ys = xs.cuda(), ys.cuda()\n",
        "\n",
        "            if is_supervised:\n",
        "                loss = cc_vae.sup(xs, ys)\n",
        "                epoch_losses_sup += loss.detach().item()\n",
        "            else:\n",
        "                loss = cc_vae.unsup(xs)\n",
        "                epoch_losses_unsup += loss.detach().item()\n",
        "\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "\n",
        "        if sup_frac != 0.0:\n",
        "            with torch.no_grad():\n",
        "                validation_accuracy = cc_vae.accuracy(data_loaders['valid'])\n",
        "        else:\n",
        "            validation_accuracy = np.nan\n",
        "\n",
        "\n",
        "        print(\"[Epoch %03d] Sup Loss %.3f, Unsup Loss %.3f, Val Acc %.3f\" %\n",
        "                (epoch, epoch_losses_sup, epoch_losses_unsup, validation_accuracy))\n",
        "    cc_vae.save_models(data_dir)\n",
        "    test_acc = cc_vae.accuracy(data_loaders['test'])\n",
        "    print(\"Test acc %.3f\" % test_acc)\n",
        "    # cc_vae.latent_traversal(img, 'data/output')\n",
        "    return\n"
      ],
      "metadata": {
        "id": "PsnJBLSKj70a"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzHqwyrBpZ72",
        "outputId": "5cf6a932-42d2-4f33-c72a-5047aed20985"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting Dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:12<00:00, 15.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 000] Sup Loss 367340.510, Unsup Loss 367673.674, Val Acc 0.609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:12<00:00, 15.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 001] Sup Loss 365907.044, Unsup Loss 365518.672, Val Acc 0.709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:12<00:00, 15.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 002] Sup Loss 358085.906, Unsup Loss 360362.001, Val Acc 0.746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:12<00:00, 15.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 003] Sup Loss 354517.341, Unsup Loss 358082.648, Val Acc 0.763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:11<00:00, 17.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 004] Sup Loss 358543.295, Unsup Loss 359070.273, Val Acc 0.782\n",
            "Test acc 0.783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "ufqUYsgl1mG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im_shape = FashionMNISTCached.shape\n",
        "prior_fn = FashionMNISTCached.prior_fn()"
      ],
      "metadata": {
        "id": "2ZbuAdgu6lah"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc_vae = CCVAE(z_dim=z_dim,\n",
        "                   num_classes=len(FashionMNIST_classes),\n",
        "                   im_shape=im_shape,\n",
        "                   use_cuda=cuda,\n",
        "                   prior_fn=prior_fn)"
      ],
      "metadata": {
        "id": "gnpO0qgt24eG"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_path = '/content/data/encoder.pt'\n",
        "decoder_path = '/content/data/decoder.pt'\n",
        "classifier_path = '/content/data/classifier.pt'\n",
        "cond_prior_path = '/content/data/cond_prior.pt'"
      ],
      "metadata": {
        "id": "bUHPL0J52P-d"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_checkpoint = torch.load(encoder_path)\n",
        "decoder_checkpoint = torch.load(decoder_path)\n",
        "classifier_checkpoint = torch.load(classifier_path)\n",
        "cond_prior_checkpoint = torch.load(cond_prior_path)"
      ],
      "metadata": {
        "id": "xPUic_r-6pSI"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc_vae.encoder.load_state_dict(encoder_checkpoint.state_dict())\n",
        "cc_vae.decoder.load_state_dict(decoder_checkpoint.state_dict())\n",
        "cc_vae.classifier.load_state_dict(classifier_checkpoint.state_dict())\n",
        "cc_vae.cond_prior.load_state_dict(cond_prior_checkpoint.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRIrxeN76s5L",
        "outputId": "49cf93a9-e3df-42f3-acb2-c8664c4b74a2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "cc_vae.encoder.eval()\n",
        "cc_vae.decoder.eval()\n",
        "cc_vae.classifier.eval()\n",
        "cc_vae.cond_prior.eval()"
      ],
      "metadata": {
        "id": "cMQAJXkT2NK_"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = FashionMNISTCached.fixed_imgs\n",
        "img = img.cuda()\n",
        "reconstructed_img = cc_vae.reconstruct_img(img).view(-1, *im_shape)"
      ],
      "metadata": {
        "id": "ipfG3VsP5BP5"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0"
      ],
      "metadata": {
        "id": "TAbnPVJW6AoD"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_np = img[index].cpu().detach().numpy()\n",
        "reconstructed_img_np = reconstructed_img[index].cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "TRurd1E-5huo"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "axes[0].imshow(img_np[0], cmap='gray')\n",
        "axes[0].set_title('Original Image')\n",
        "axes[1].imshow(reconstructed_img_np[0], cmap='gray')\n",
        "axes[1].set_title('Reconstructed Image')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "Jjwhrd7R6HEO",
        "outputId": "fb7835d9-c9fc-47a3-a80b-1c943b9090c9"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4-xZBGOr935Z"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}